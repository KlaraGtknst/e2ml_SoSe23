{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c70fa63-b796-4b94-b670-0e98bb66a5a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Design of Experiments with and for Machine Learning\n",
    "\n",
    "In this notebook, we will implement **Bayesian optimization** techniques as adative sampling methodes for experimental design. \n",
    "\n",
    "At the start, we will implement a *Gaussian process* (GP) for regression problems.\n",
    "\n",
    "Subsequently, we will use GPs as surrogate probabilistic models to implement the acquisition functions\n",
    "- *probability improvement* (PI),\n",
    "- *expected improvement* (EI),\n",
    "- and *upper confidence bound* (UBC).\n",
    "\n",
    "Finally, we will employ the implemented acquisition functions on synthethic data sets to study their behaviors.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Gaussian Processes for Regression](#gpr)\n",
    "2. [Acquisition Functions](#af)\n",
    "3. [Bayesian Optimization](#bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbb5690-6d49-42df-bf4b-db120f76c893",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interactive, FloatSlider, IntSlider, Dropdown\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e495a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **1. Gaussian Processes for Regression** <a class=\"anchor\" id=\"gpr\"></a>\n",
    "\n",
    "**Kernel functions** are central components of Gaussian processes. A kernel function $k: \\mathcal{X} \\rightarrow \\mathbb{R}$ is typically chosen to quantify a kind of similarity between two arbitrary instances $\\mathbf{x}_n$ and $\\mathbf{x}_m$. This similary measurement corresponds to a dot product of two instances in a transformed Hilbert space:\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}^\\prime) = \\boldsymbol{\\phi}(\\mathbf{x})^\\mathrm{T} \\boldsymbol{\\phi}(\\mathbf{x}^\\prime),\n",
    "$$\n",
    "where $\\boldsymbol{\\phi}: \\mathcal{X} \\rightarrow \\mathcal{H}$ is the feature map of the corresponding kernel function. Accordingly, we can use this property to prove whether a function defines a valid kernel. As a simple example, consider a kernel function given by\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}^\\prime) = (\\mathbf{x}^\\mathrm{T}\\mathbf{x}^\\prime)^2.\n",
    "$$\n",
    "If we take the particular case of a two-dimensional instance space $\\mathcal{X} = \\mathbb{R}^2$ we can expand out the terms and thereby identify the following nonlinear feature mapping:\n",
    "$$\n",
    "(x[0] * x'[0] + x[1] * x'[1])^2 = (x[0]^2 * x'[0]^2 + 2*x[0]*x[0]'*x[1]*x[1]'+ x[1]^2 * x'[1]^2)\n",
    "= [x[0]^2, \\sqrt{2} * x[0] * x[1], x[1]^2 ] * [x[0]'^2, \\sqrt{2} * x[0]' * x[1]', x[1]'^2]^T = \\phi(x)^T * \\phi(x')\n",
    "$$\n",
    "There are several kernel functions, which can be used in combination with a Gaussian process regression model. \n",
    "For simplicity, we will use the [`pairwise_kernels`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_kernels.html) as part of the scikit-learn package. In the following, we see different functions $f$ drawn from the Gaussian prior $\\mathcal{N}(\\mathbf{f} \\mid \\mathbf{0}, \\mathbf{K})$, where the Gram matrix $\\mathbf{K} \\in \\mathbb{R}^{N \\times N}$ is computed through the radial basis function (RBF) kernel, also known as Gaussian kernel, defined as:\n",
    "$$\n",
    "k(x, x') = exp(-\\gamma ||(x-x')||^2)\n",
    "$$\n",
    "\\gamma weights effect of distance of two points. small -> larger effect on other values/ knowing sth about a value of the random value at a certain point gives much information about a point farer away -> linear lines, big -> smaller effect on other values -> many max and min/ hügelige Kurven.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037e06dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22b339babe04ed0aa8189d6cc55e8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='gamma', max=10.0, min=0.01), Output()), _dom_classes…"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "def visualize_prior_functions(gamma):\n",
    "    # Create samples (instances).\n",
    "    x_axis = np.linspace(-1, 1, 100)\n",
    "    X = x_axis.reshape(-1, 1)\n",
    "\n",
    "    # Compute Gram matrix `K` using RBF kernel\n",
    "    # with `gamma`.\n",
    "    K = pairwise_kernels(X, metric='rbf', gamma= gamma)\n",
    "\n",
    "    # Draw 5 function values `f` ~ N(0, K).\n",
    "    functions = np.random.multivariate_normal(mean=np.zeros_like(x_axis), cov=K, size=5).T\n",
    "\n",
    "    # Plot drawn samples `f`.\n",
    "    plt.plot(x_axis, functions)\n",
    "    plt.ylim([-5, 5])\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_prior_functions, \n",
    "    gamma=FloatSlider(value=1, min=0.01, max=10), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8faa8b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The key results defining a Gaussian process are given through the mean and variance of the conditional distribution $p(y_{N+1} \\mid \\mathcal{D}_N)$. The corresponding formulas are given in the following:\n",
    "\n",
    "Folie 36\n",
    "mean:\n",
    "$$\n",
    " \\mu(x_{N+1} \\mid \\mathcal{D}_N) = \\textbf{k}^T * \\textbf{C}^{-1}_N * \\textbf{y}\n",
    "$$\n",
    "\n",
    "variance:\n",
    "$$\n",
    " \\sigma^2(x_{N+1} \\mid \\mathcal{D}_N) = c - \\mu(x_{N+1} \\mid \\mathcal{D}_N)\n",
    "$$\n",
    "\n",
    "With this knowledge, we implement the class [`GaussianProcessRegression`](../e2ml/models/_gaussian_process_regression.py) in the [`e2ml.models`](../e2ml/models) subpackage.\n",
    "Once, the implementation has been completed, we check its validity on a simple one-dimensional artificial data set through visual inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e3fb0a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e4925d0b4244019aaf9cd483dd37c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='noise', max=10.0), FloatSlider(value=0.01, descripti…"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2ml.models import GaussianProcessRegression\n",
    "\n",
    "def objective_function(x, noise, random_state=3):\n",
    "    # noise = np.random.normal(loc=0, scale=noise, size=len(x)) if noise > 0 else 0\n",
    "    # Keep the same noise for each run.\n",
    "    r = np.random.RandomState(random_state)\n",
    "    noise = r.normal(loc=0, scale=noise, size=len(x)) if noise > 0 else 0\n",
    "    return (np.sin(2 * np.pi * x)**6.0 * x) + noise\n",
    "\n",
    "def generate_data(noise, train_ratio, random_state=3):\n",
    "    x_axis = np.linspace(-1, 1, 100)\n",
    "    X = x_axis.reshape(-1, 1)\n",
    "    n_train_samples = int(train_ratio * len(X))\n",
    "    # train_idx = np.random.choice(np.arange(len(X)), replace=False, size=n_train_samples)\n",
    "    # Keep the same training samples for each run.\n",
    "    r = np.random.RandomState(random_state)\n",
    "    train_idx = r.choice(np.arange(len(X)), replace=False, size=n_train_samples)\n",
    "    X_train = X[train_idx]\n",
    "    y_train = objective_function(X_train.ravel(), noise=noise)\n",
    "    f = objective_function(x_axis, noise=0)\n",
    "    return x_axis, X, f, X_train, y_train\n",
    "\n",
    "def visualize_gpr_predictions(noise=0.1, beta=0.1, gamma=1, train_ratio=0.5):\n",
    "    # Create samples (instances) and targets.\n",
    "    x_axis, X, f, X_train, y_train = generate_data(noise, train_ratio)\n",
    "    \n",
    "    # Create Gaussian process.\n",
    "    metrics_dict = {'gamma': gamma, 'metric': 'rbf'}\n",
    "    gpr = GaussianProcessRegression(beta=beta, metrics_dict=metrics_dict)\n",
    "    \n",
    "    # Fit Gaussian process on training data.\n",
    "    gpr.fit(X_train, y=y_train)\n",
    "\n",
    "    # Predict `means` and `stds`.\n",
    "    means, stds = gpr.predict(X, return_std=True)\n",
    "    \n",
    "    # Visualize targets and predictions.\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.scatter(X_train.ravel(), y_train, s=20, c='k')\n",
    "    plt.plot(x_axis, f, label=\"$f(x)$\", c='r', ls='--')\n",
    "    plt.plot(x_axis, means, label=\"$\\mu(x)$\", c='b')\n",
    "    plt.fill_between(x_axis, means-stds, means+stds, alpha=0.1, color='b')\n",
    "    plt.xlabel('$x$', fontsize=15)\n",
    "    plt.ylabel('$f(x)$', fontsize=15)\n",
    "    plt.legend(prop={'size': 10})\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_gpr_predictions, \n",
    "    noise=FloatSlider(value=0.1, min=0.0, max=10),\n",
    "    beta=FloatSlider(value=0.01, min=0.01, max=2),  # beta should never be 0\n",
    "    gamma=FloatSlider(value=50, min=0.01, max=100),\n",
    "    train_ratio=FloatSlider(value=0.5, min=0.1, max=1)\n",
    ")\n",
    "\n",
    "# gamma low -> less hügelig, linear fitting, lower polynomial,\n",
    "# gamma high -> higher polynom, overfitting\n",
    "\n",
    "# beta high -> high uncertainty\n",
    "\n",
    "# noise high -> worse modeling\n",
    "\n",
    "# linear kernel in metrics_dict tries to fit linear to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b78d5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **2. Acquisition Functions** <a class=\"anchor\" id=\"af\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ba74c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Three popular acquisition functions are:\n",
    "- probability improvement (PI) defined as:\n",
    "$$\n",
    "\\alpha_{PI}(\\bold{x}|\\mathcal{D_N}) = \\Phi(\\frac{\\mu(\\bold{x}|\\mathcal{D_N})-\\tau}{\\sigma(\\bold{x}|\\mathcal{D_N})})\n",
    "$$\n",
    "\n",
    "- expected improvement (EI) defined as:\n",
    "\n",
    "$$\n",
    "\\alpha_{EI}(\\bold{x}|\\mathcal{D_N}) = (\\mu(\\bold{x}|\\mathcal{D_N})-\\tau) \\Phi(\\frac{\\mu(\\bold{x}|\\mathcal{D_N})-\\tau}{\\sigma(\\bold{x}|\\mathcal{D_N})}) + \\sigma(\\bold{x}|\\mathcal{D_N}) \\phi(\\frac{\\mu(\\bold{x}|\\mathcal{D_N})-\\tau}{\\sigma(\\bold{x}|\\mathcal{D_N})})\n",
    "$$\n",
    "\n",
    "- and upper confidence bound (UCB) defined as:\n",
    "$$\n",
    "\\alpha_{UCB}(\\bold{x}|\\mathcal{D_N}) = \\mu(\\bold{x}|\\mathcal{D_N}) + \\kappa_N\\sigma(\\bold{x}|\\mathcal{D_N})\n",
    "$$\n",
    "\n",
    "First two: given current model, which future sample improves tau/model the most\n",
    "EI considers high variance and high mean (broad CDF)\n",
    "PI for finding local minimum, less Exploration\n",
    "Last one: Exploration, Sigma rule, large kappa -> higher variance impact, low kappa -> consider high mean regions. Set Kappa high, then reduce it (first exploration, then exploitation)\n",
    "\n",
    "#### Question:\n",
    "2. (a) What is the task of an acquisition function?\n",
    "\n",
    "   Evaluate best option for next sample input of experiment.\n",
    "   Exploration and exploitation trade-off.\n",
    "   \n",
    "With this knowledge, we implement the functions \n",
    "\n",
    "- [`acquisition_pi`](../e2ml/experimentation/_bayesian_optimization.py),\n",
    "- [`acquisition_ei`](../e2ml/experimentation/_bayesian_optimization.py),\n",
    "- and [`acquisition_ucb`](../e2ml/experimentation/_bayesian_optimization.py)\n",
    "\n",
    "in the [`e2ml.experimentation`](../e2ml/experimentation) subpackage.\n",
    "\n",
    "Once, the implementations have been completed, we check their validity on a simple one-dimensional artificial data set through visual inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffd67ada",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0426945755a84b639a63db5ab97a0788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='noise', max=2.0), FloatSlider(value=0.01, descriptio…"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2ml.experimentation import acquisition_pi, acquisition_ei, acquisition_ucb\n",
    "\n",
    "def visualize_acquisition_functions(noise=0.1, beta=0.1, gamma=1, train_ratio=0.1, kappa=1.):\n",
    "    # Create samples (instances) and targets.\n",
    "    x_axis, X, f, X_train, y_train = generate_data(noise, train_ratio)\n",
    "    \n",
    "    # Create Gaussian process.\n",
    "    metrics_dict = {'gamma': gamma, 'metric': 'rbf'}\n",
    "    gpr = GaussianProcessRegression(beta=beta, metrics_dict=metrics_dict)\n",
    "    \n",
    "    # Fit Gaussian process on training data.\n",
    "    gpr.fit(X_train, y_train)\n",
    "\n",
    "    # Predict `means` and `stds`.\n",
    "    means, stds = gpr.predict(X, return_std=True)\n",
    "    \n",
    "    # Determine `tau`.\n",
    "    tau = np.max(y_train) # find values better than tau\n",
    "    \n",
    "    # Compute PI scores as `pi_scores`.\n",
    "    pi_scores = acquisition_pi(mu=means, sigma=stds, tau=tau)\n",
    "    \n",
    "    # Compute EI scores as `ei_scores`.\n",
    "    ei_scores = acquisition_ei(mu=means, sigma=stds, tau=tau)\n",
    "    \n",
    "    # Compute UCB scores as `ucb_scores` with `kappa`.\n",
    "    ucb_scores = acquisition_ucb(means, sigma=stds, kappa=kappa)\n",
    "    \n",
    "    # Visualize targets and predictions.\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.scatter(X_train.ravel(), y_train, s=20, c='k')\n",
    "    plt.plot(x_axis, f, label=\"$f(x)$\", c='r', ls='--')\n",
    "    plt.plot(x_axis, means, label=\"$\\mu(x)$\", c='b')\n",
    "    plt.plot(x_axis, pi_scores, label='PI', c='g')\n",
    "    plt.plot(x_axis, ei_scores, label='EI', c='g', ls='--')\n",
    "    plt.plot(x_axis, ucb_scores, label='UCB', c='g', ls=':')\n",
    "    plt.fill_between(x_axis, means-stds, means+stds, alpha=0.1, color='b')\n",
    "    plt.xlabel('$x$', fontsize=15)\n",
    "    plt.ylabel('$f(x)$', fontsize=15)\n",
    "    plt.legend(prop={'size': 12})\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_acquisition_functions, \n",
    "    noise=FloatSlider(value=0.1, min=0.0, max=2),\n",
    "    beta=FloatSlider(value=0.01, min=0.01, max=2),\n",
    "    gamma=FloatSlider(value=50, min=0.01, max=100),\n",
    "    train_ratio=FloatSlider(value=0.01, min=0.01, max=1, step=0.01),\n",
    "    kappa=FloatSlider(value=1, min=0.0, max=3)\n",
    ")\n",
    "\n",
    "# Kappa 1 == upper shape of uncertainty\n",
    "# look at jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce9ae3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **3. Bayesian Optimization** <a class=\"anchor\" id=\"bo\"></a>\n",
    "\n",
    "Since we have implemented the Gaussian process as surrogate probabilistic model and possible acquisition functions, we can perform Bayesian optimization. For this purpose, we implement the function [`perform_bayesian_optimization`](../e2ml/experimentation/_bayesian_optimization.py) in the [`e2ml.experimentation`](../e2ml/experimentation) subpackage. Once, the implementation has been completed, we test it on a simple one-dimensional artificial data set through visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c9f52f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32bbe7b11234e17b1c96ac03969aa40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=50.0, description='gamma', min=0.01), FloatSlider(value=0.1, descripti…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2ml.experimentation import perform_bayesian_optimization\n",
    "\n",
    "def visualize_bayesian_optimization(gamma=50, noise=0.05, beta=0.05, n_evals=100,\n",
    "                                    n_random_init=5, acquisition_function='pi'):\n",
    "    # Generate data and true objective values.\n",
    "    x_axis = np.linspace(-1, 1, 1000)\n",
    "    X_cand = x_axis.reshape(-1, 1)\n",
    "    f = objective_function(x_axis, noise=0)\n",
    "    \n",
    "    # Partially initialize objective_function.\n",
    "    obj_func = partial(objective_function, noise=noise)\n",
    "    \n",
    "    # Create Gaussian process model.\n",
    "    metrics_dict = {'gamma': gamma, 'metric': 'rbf'}\n",
    "    gpr = GaussianProcessRegression(beta=beta, metrics_dict=metrics_dict)\n",
    "    \n",
    "    # Perform Bayesian optimization with given parameters\n",
    "    # to obtain `X_acquired` and `y_acquired`.\n",
    "    X_acquired, y_acquired = perform_bayesian_optimization(X_cand, gpr, acquisition_function, obj_func, n_evals, n_random_init)\n",
    "    \n",
    "    # Fit Gaussian process as `gpr` on acquired data.\n",
    "    gpr.fit(X_acquired, y_acquired)\n",
    "    \n",
    "    # Predict `means` and `stds` on `X_cand`.\n",
    "    means, stds = gpr.predict(X_cand)\n",
    "    \n",
    "    # Visualize Bayesian optimization process.\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.plot(x_axis, means, label=\"$\\mu(x)$\", c='b')\n",
    "    plt.fill_between(x_axis, means-stds, means+stds, alpha=0.1, color='b')\n",
    "    plt.plot(x_axis, f, label=\"$f(x)$\", c='r', ls='--')\n",
    "    plt.scatter(X_acquired.ravel(), y_acquired, s=20, c='k', label='acquired samples')\n",
    "    for i in range(n_evals):\n",
    "        plt.text(X_acquired[i], y_acquired[i], str(i+1), color=\"k\", fontsize=12)\n",
    "    plt.xlabel('$x$', fontsize=15)\n",
    "    plt.ylabel('$f(x)$', fontsize=15)\n",
    "    plt.legend(prop={'size': 12})\n",
    "    plt.title('Bayesian optimization with numbers indicating acquisition order', fontsize=15)\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_bayesian_optimization, \n",
    "    gamma=FloatSlider(value=50, min=0.01, max=100),\n",
    "    noise=FloatSlider(value=0.1, min=0.0, max=10),\n",
    "    beta=FloatSlider(value=0.01, min=0.0, max=2),\n",
    "    n_evals=IntSlider(value=20, min=1, max=500),\n",
    "    n_random_init=IntSlider(value=5, min=1, max=500),\n",
    "    acquisition_function=Dropdown(options=['pi', 'ei', 'ucb'], value='pi')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45062504",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}