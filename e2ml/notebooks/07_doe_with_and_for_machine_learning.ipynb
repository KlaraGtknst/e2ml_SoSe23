{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c70fa63-b796-4b94-b670-0e98bb66a5a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Design of Experiments with and for Machine Learning\n",
    "\n",
    "In this notebook, we will implement **Bayesian optimization** techniques as adative sampling methodes for experimental design. \n",
    "\n",
    "At the start, we will implement a *Gaussian process* (GP) for regression problems.\n",
    "\n",
    "Subsequently, we will use GPs as surrogate probabilistic models to implement the acquisition functions\n",
    "- *probability improvement* (PI),\n",
    "- *expected improvement* (EI),\n",
    "- and *upper confidence bound* (UBC).\n",
    "\n",
    "Finally, we will employ the implemented acquisition functions on synthethic data sets to study their behaviors.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Gaussian Processes for Regression](#gpr)\n",
    "2. [Acquisition Functions](#af)\n",
    "3. [Bayesian Optimization](#bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbb5690-6d49-42df-bf4b-db120f76c893",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interactive, FloatSlider, IntSlider, Dropdown\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e495a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **1. Gaussian Processes for Regression** <a class=\"anchor\" id=\"gpr\"></a>\n",
    "\n",
    "**Kernel functions** are central components of Gaussian processes. A kernel function $k: \\mathcal{X} \\rightarrow \\mathbb{R}$ is typically chosen to quantify a kind of similarity between two arbitrary instances $\\mathbf{x}_n$ and $\\mathbf{x}_m$. This similary measurement corresponds to a dot product of two instances in a transformed Hilbert space:\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}^\\prime) = \\boldsymbol{\\phi}(\\mathbf{x})^\\mathrm{T} \\boldsymbol{\\phi}(\\mathbf{x}^\\prime),\n",
    "$$\n",
    "where $\\boldsymbol{\\phi}: \\mathcal{X} \\rightarrow \\mathcal{H}$ is the feature map of the corresponding kernel function. Accordingly, we can use this property to prove whether a function defines a valid kernel. As a simple example, consider a kernel function given by\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}^\\prime) = (\\mathbf{x}^\\mathrm{T}\\mathbf{x}^\\prime)^2.\n",
    "$$\n",
    "If we take the particular case of a two-dimensional instance space $\\mathcal{X} = \\mathbb{R}^2$ we can expand out the terms and thereby identify the following nonlinear feature mapping:\n",
    "$$\n",
    "(x[0] * x'[0] + x[1] * x'[1])^2\n",
    "$$\n",
    "There are several kernel functions, which can be used in combination with a Gaussian process regression model. \n",
    "For simplicity, we will use the [`pairwise_kernels`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_kernels.html) as part of the scikit-learn package. In the following, we see different functions $f$ drawn from the Gaussian prior $\\mathcal{N}(\\mathbf{f} \\mid \\mathbf{0}, \\mathbf{K})$, where the Gram matrix $\\mathbf{K} \\in \\mathbb{R}^{N \\times N}$ is computed through the radial basis function (RBF) kernel, also known as Gaussian kernel, defined as:\n",
    "$$\n",
    "k(x, x') = exp(\\frac{-(x-x')^2}{2*\\sigma^2})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037e06dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(FloatSlider(value=1.0, description='gamma', max=10.0, min=0.01), Output()), _dom_classes…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af69e495b9a24a5d998aedd5c589c8f3"
      }
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "def visualize_prior_functions(gamma):\n",
    "    # Create samples (instances).\n",
    "    x_axis = np.linspace(-1, 1, 100)\n",
    "    X = x_axis.reshape(-1, 1)\n",
    "\n",
    "    # Compute Gram matrix `K` using RBF kernel\n",
    "    # with `gamma`.\n",
    "    K = 1.0 *  rbf_kernel(X, gamma = gamma)\n",
    "\n",
    "    # Draw 5 function values `f` ~ N(0, K).\n",
    "    functions = []\n",
    "    for i in range(5):\n",
    "        functions[i] = np.random.normal(loc = 0.0, scale = K, size = len(x_axis))\n",
    "\n",
    "        # Plot drawn samples `f`.\n",
    "        plt.plot(x_axis, functions[i], label=f\"function {i}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_prior_functions, \n",
    "    gamma=FloatSlider(value=1, min=0.01, max=10), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8faa8b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The key results defining a Gaussian process are given through the mean and variance of the conditional distribution $p(y_{N+1} \\mid \\mathcal{D}_N)$. The corresponding formulas are given in the following:\n",
    "\n",
    "TODO\n",
    "\n",
    "With this knowledge, we implement the class [`GaussianProcessRegression`](../e2ml/models/_gaussian_process_regression.py) in the [`e2ml.models`](../e2ml/models) subpackage.\n",
    "Once, the implementation has been completed, we check its validity on a simple one-dimensional artificial data set through visual inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3fb0a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(FloatSlider(value=0.1, description='noise', max=10.0), FloatSlider(value=0.01, descripti…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb705db9f196440daac00d86d25663dc"
      }
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2ml.models import GaussianProcessRegression\n",
    "\n",
    "def objective_function(x, noise):\n",
    "    noise = np.random.normal(loc=0, scale=noise, size=len(x)) if noise > 0 else 0\n",
    "    return (np.sin(2 * np.pi * x)**6.0 * x) + noise\n",
    "\n",
    "def generate_data(noise, train_ratio):\n",
    "    x_axis = np.linspace(-1, 1, 100)\n",
    "    X = x_axis.reshape(-1, 1)\n",
    "    n_train_samples = int(train_ratio * len(X))\n",
    "    train_idx = np.random.choice(np.arange(len(X)), replace=False, size=n_train_samples)\n",
    "    X_train = X[train_idx]\n",
    "    y_train = objective_function(X_train.ravel(), noise=noise)\n",
    "    f = objective_function(x_axis, noise=0)\n",
    "    return x_axis, X, f, X_train, y_train\n",
    "\n",
    "def visualize_gpr_predictions(noise=0.1, beta=0.1, gamma=1, train_ratio=0.5):\n",
    "    # Create samples (instances) and targets.\n",
    "    x_axis, X, f, X_train, y_train = generate_data(noise, train_ratio)\n",
    "    \n",
    "    # Create Gaussian process.\n",
    "    metrics_dict = {'gamma': gamma, 'metric': 'rbf'}\n",
    "    gpr = GaussianProcessRegression(beta=beta, metrics_dict=metrics_dict)\n",
    "    \n",
    "    # Fit Gaussian process on training data.\n",
    "    gpr.fit(X_train, y=y_train)\n",
    "\n",
    "    # Predict `means` and `stds`.\n",
    "    means, stds = gpr.predict(X, True)\n",
    "    \n",
    "    # Visualize targets and predictions.\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.scatter(X_train.ravel(), y_train, s=20, c='k')\n",
    "    plt.plot(x_axis, f, label=\"$f(x)$\", c='r', ls='--')\n",
    "    plt.plot(x_axis, means, label=\"$\\mu(x)$\", c='b')\n",
    "    plt.fill_between(x_axis, means-stds, means+stds, alpha=0.1, color='b')\n",
    "    plt.xlabel('$x$', fontsize=15)\n",
    "    plt.ylabel('$f(x)$', fontsize=15)\n",
    "    plt.legend(prop={'size': 10})\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_gpr_predictions, \n",
    "    noise=FloatSlider(value=0.1, min=0.0, max=10),\n",
    "    beta=FloatSlider(value=0.01, min=0.0, max=2),\n",
    "    gamma=FloatSlider(value=50, min=0.01, max=100),\n",
    "    train_ratio=FloatSlider(value=0.5, min=0.1, max=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b78d5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **2. Acquisition Functions** <a class=\"anchor\" id=\"af\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ba74c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Three popular acquisition functions are:\n",
    "- probability improvement (PI) defined as:\n",
    "$$\n",
    "\\alpha_{PI}(\\bold{x}|\\mathcal{D_N}) = \\Phi(\\frac{\\mu(\\bold{x}|\\mathcal{D_N})-\\tau}{\\sigma(\\bold{x}|\\mathcal{D_N})})\n",
    "$$\n",
    "\n",
    "- expected improvement (EI) defined as:\n",
    "$$\n",
    "\\alpha_{EI}(\\bold{x}|\\mathcal{D_N}) = (\\mu(\\bold{x}|\\mathcal{D_N})-\\tau) \\Phi(\\frac{\\mu(\\bold{x}|\\mathcal{D_N})-\\tau}{\\sigma(\\bold{x}|\\mathcal{D_N})}) + \\sigma(\\bold{x}|\\mathcal{D_N}) \\phi(\\frac{\\mu(\\bold{x}|\\mathcal{D_N})-\\tau}{\\sigma(\\bold{x}|\\mathcal{D_N})})\n",
    "$$\n",
    "\n",
    "- and upper confidence bound (UCB) defined as:\n",
    "$$\n",
    "\\alpha_{UCB}(\\bold{x}|\\mathcal{D_N}) = \\mu(\\bold{x}|\\mathcal{D_N}) + \\kappa_N\\sigma(\\bold{x}|\\mathcal{D_N})\n",
    "$$\n",
    "\n",
    "#### Question:\n",
    "2. (a) What is the task of an acquisition function?\n",
    "\n",
    "   Evaluate best option for next sample input of experiment.\n",
    "   Exploration and exploitation trade-off.\n",
    "   \n",
    "With this knowledge, we implement the functions \n",
    "\n",
    "- [`acquisition_pi`](../e2ml/experimentation/_bayesian_optimization.py),\n",
    "- [`acquisition_ei`](../e2ml/experimentation/_bayesian_optimization.py),\n",
    "- and [`acquisition_ucb`](../e2ml/experimentation/_bayesian_optimization.py)\n",
    "\n",
    "in the [`e2ml.experimentation`](../e2ml/experimentation) subpackage.\n",
    "\n",
    "Once, the implementations have been completed, we check their validity on a simple one-dimensional artificial data set through visual inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd67ada",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(FloatSlider(value=0.1, description='noise', max=2.0), FloatSlider(value=0.01, descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f16e89d340447cc8e151b7092360249"
      }
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2ml.experimentation import acquisition_pi, acquisition_ei, acquisition_ucb\n",
    "\n",
    "def visualize_acquisition_functions(noise=0.1, beta=0.1, gamma=1, train_ratio=0.1, kappa=1.):\n",
    "    # Create samples (instances) and targets.\n",
    "    x_axis, X, f, X_train, y_train = generate_data(noise, train_ratio)\n",
    "    \n",
    "    # Create Gaussian process.\n",
    "    metrics_dict = {'gamma': gamma, 'metric': 'rbf'}\n",
    "    gpr = GaussianProcessRegression(beta=beta, metrics_dict=metrics_dict)\n",
    "    \n",
    "    # Fit Gaussian process on training data.\n",
    "    gpr.fit(X_train, y_train)\n",
    "\n",
    "    # Predict `means` and `stds`.\n",
    "    means, stds = gpr.predict(X, True)\n",
    "    \n",
    "    # Determine `tau`.\n",
    "    tau = 0 # find values better than tau\n",
    "    \n",
    "    # Compute PI scores as `pi_scores`.\n",
    "    pi_scores = acquisition_pi(means, stds, tau)\n",
    "    \n",
    "    # Compute EI scores as `ei_scores`.\n",
    "    ei_scores = acquisition_ei(means, stds, tau)\n",
    "    \n",
    "    # Compute UCB scores as `ucb_scores` with `kappa`.\n",
    "    ucb_scores = acquisition_ucb(means, stds, kappa)\n",
    "    \n",
    "    # Visualize targets and predictions.\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.scatter(X_train.ravel(), y_train, s=20, c='k')\n",
    "    plt.plot(x_axis, f, label=\"$f(x)$\", c='r', ls='--')\n",
    "    plt.plot(x_axis, means, label=\"$\\mu(x)$\", c='b')\n",
    "    plt.plot(x_axis, pi_scores, label='PI', c='g')\n",
    "    plt.plot(x_axis, ei_scores, label='EI', c='g', ls='--')\n",
    "    plt.plot(x_axis, ucb_scores, label='UCB', c='g', ls=':')\n",
    "    plt.fill_between(x_axis, means-stds, means+stds, alpha=0.1, color='b')\n",
    "    plt.xlabel('$x$', fontsize=15)\n",
    "    plt.ylabel('$f(x)$', fontsize=15)\n",
    "    plt.legend(prop={'size': 12})\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_acquisition_functions, \n",
    "    noise=FloatSlider(value=0.1, min=0.0, max=2),\n",
    "    beta=FloatSlider(value=0.01, min=0.0, max=2),\n",
    "    gamma=FloatSlider(value=50, min=0.01, max=100),\n",
    "    train_ratio=FloatSlider(value=0.05, min=0.05, max=1),\n",
    "    kappa=FloatSlider(value=1, min=0.0, max=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce9ae3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **3. Bayesian Optimization** <a class=\"anchor\" id=\"bo\"></a>\n",
    "\n",
    "Since we have implemented the Gaussian process as surrogate probabilistic model and possible acquisition functions, we can perform Bayesian optimization. For this purpose, we implement the function [`perform_bayesian_optimization`](../e2ml/experimentation/_bayesian_optimization.py) in the [`e2ml.experimentation`](../e2ml/experimentation) subpackage. Once, the implementation has been completed, we test it on a simple one-dimensional artificial data set through visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32f0f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from e2ml.experimentation import perform_bayesian_optimization\n",
    "\n",
    "def visualize_bayesian_optimization(gamma=50, noise=0.05, beta=0.05, n_evals=100,\n",
    "                                    n_random_init=5, acquisition_function='pi'):\n",
    "    # Generate data and true objective values.\n",
    "    x_axis = np.linspace(-1, 1, 1000)\n",
    "    X_cand = x_axis.reshape(-1, 1)\n",
    "    f = objective_function(x_axis, noise=0)\n",
    "    \n",
    "    # Partially initialize objective_function.\n",
    "    obj_func = partial(objective_function, noise=noise)\n",
    "    \n",
    "    # Create Gaussian process model.\n",
    "    metrics_dict = {'gamma': gamma, 'metric': 'rbf'}\n",
    "    gpr = GaussianProcessRegression(beta=beta, metrics_dict=metrics_dict)\n",
    "    \n",
    "    # Perform Bayesian optimization with given parameters\n",
    "    # to obtain `X_acquired` and `y_acquired`.\n",
    "    X_acquired, y_acquired = gpr.perform_bayesian_optimization(X_cand, gpr, acquisition_function, obj_func, n_evals, n_random_init)\n",
    "    \n",
    "    # Fit Gaussian process as `gpr` on acquired data.\n",
    "    gpr.fit(X_acquired, y_acquired)\n",
    "    \n",
    "    # Predict `means` and `stds` on `X_cand`.\n",
    "    means, stds = gpr.predict(X_cand)\n",
    "    \n",
    "    # Visualize Bayesian optimization process.\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.plot(x_axis, means, label=\"$\\mu(x)$\", c='b')\n",
    "    plt.fill_between(x_axis, means-stds, means+stds, alpha=0.1, color='b')\n",
    "    plt.plot(x_axis, f, label=\"$f(x)$\", c='r', ls='--')\n",
    "    plt.scatter(X_acquired.ravel(), y_acquired, s=20, c='k', label='acquired samples')\n",
    "    for i in range(n_evals):\n",
    "        plt.text(X_acquired[i], y_acquired[i], str(i+1), color=\"k\", fontsize=12)\n",
    "    plt.xlabel('$x$', fontsize=15)\n",
    "    plt.ylabel('$f(x)$', fontsize=15)\n",
    "    plt.legend(prop={'size': 12})\n",
    "    plt.title('Bayesian optimization with numbers indicating acquisition order', fontsize=15)\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_bayesian_optimization, \n",
    "    gamma=FloatSlider(value=50, min=0.01, max=100),\n",
    "    noise=FloatSlider(value=0.1, min=0.0, max=10),\n",
    "    beta=FloatSlider(value=0.01, min=0.0, max=2),\n",
    "    n_evals=IntSlider(value=20, min=1, max=500),\n",
    "    n_random_init=IntSlider(value=5, min=1, max=500),\n",
    "    acquisition_function=Dropdown(options=['pi', 'ei', 'ucb'], value='pi')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}